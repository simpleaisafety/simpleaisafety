- Outer Misalignment
- Inner Misalignment
- Mesa Optimizer
- Reward Misspecification
- Reinforcement Learning from Human Feedback (RLHF)
- Reward Tampering
- Reward Hacking
- Reinforcement Learning
- Mechanistic Interpretability
- Alignment
- Value Learning
- Corrigibility
- Optimization
- Takeoff
- Timelines
- P/doom
- Deception
- Agent Foundations
- Scalable Oversight
- Robustness
- Evaluations (evals)
- Adversarial training
- Superalignment
- Constitutional AI
- Natural abstractions
- Shard theory
