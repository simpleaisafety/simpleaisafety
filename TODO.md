- Outer Misalignment
- Inner Misalignment
- Mesa Optimizer
- Reward Misspecification
- Reinforcement Learning from Human Feedback (RLHF)
- Reward Tampering
- Reward Hacking
- Reinforcement Learning
- Mechanistic Interpretability
- Alignment
- Value Learning
- Corrigibility
- Optimization
- Takeoff
- Timelines