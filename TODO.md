- Outer Misalignment > partially complete
- Inner Misalignment > partially complete
- Mesa Optimizer
- Reward Misspecification > partially complete
- Reinforcement Learning from Human Feedback (RLHF)
- Reward Tampering > partially complete
- Reward Hacking > partially complete
- Reinforcement Learning
- Mechanistic Interpretability
- Alignment
- Value Learning
- Corrigibility
- Optimization
- Takeoff
- Timelines
- P/doom
- Deception
- Agent Foundations
- Scalable Oversight
- Robustness
- Evaluations (evals)
- Adversarial training
- Superalignment
- Constitutional AI
- Natural abstractions
- Overfitting
- Wireheading
- Goodhart's Law
- Responsible Scaling Policies
- Shard theory
