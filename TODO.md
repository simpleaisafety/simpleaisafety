- Outer Misalignment > partially complete
- Inner Misalignment > partially complete
- Mesa Optimizer > partially complete
- Reward Misspecification > partially complete
- Reinforcement Learning from Human Feedback (RLHF) > partially complete
- Reward Tampering > partially complete
- Reward Hacking > partially complete
- Reinforcement Learning > partially complete
- Mechanistic Interpretability > partially complete
- Alignment
- Value Learning
- Corrigibility
- Optimization
- Takeoff
- Timelines
- P/doom
- Deception
- Agent Foundations
- Scalable Oversight
- Robustness
- Evaluations (evals)
- Adversarial training
- Superalignment
- Constitutional AI
- Natural abstractions
- Shard theory
- Overfitting
- Wireheading
- Goodhart's Law
- Responsible Scaling Policies
- Fine-tuning
- Inverse Reinforcement Learning (IRL)
