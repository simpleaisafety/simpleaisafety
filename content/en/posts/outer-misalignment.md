Intermediate

At its core, outer misalignment deals with the mismatch between what humans intend for an AI system to do and the objective that is actually given to the AI.
Imagine you're training a dog. You want the dog to fetch the newspaper for you, but instead of training it to do just that, you unintentionally teach it to fetch any paper-related item, including your important documents. This is a simple analogy for outer misalignment â€“ the dog's action doesn't align with your true intentions.
When we specify an objective for a machine learning model, we hope it understands and follows our intent. However, the model is only as good as the explicit instructions we provide. If there's a discrepancy between our intent and the specified goal, the AI might act in ways that are unexpected or harmful.
For instance, consider a hypothetical AI tasked with minimizing the number of emails in an inbox. If not specified correctly, the AI might simply delete all emails (including the important ones) to achieve a 'zero' inbox count. The AI technically achieved its objective, but it was not aligned with the human user's true desires.

Relation to Other Terms

While outer misalignment deals with the mismatch between human intent and the AI's specified objective, inner alignment deals with the discrepancy between the AI's specified objective and the AI's learned behavior. In our dog analogy, if the dog fetches the newspaper but then decides to chew it up (despite not being told to do so), it exhibits inner misalignment.
Reward misspecification is a subset of outer misalignment and pertains specifically to situations where the reward function (or the feedback mechanism) for an AI isn't properly defined. If the rewards provided to the AI don't encourage the right behaviors, the AI may optimize for unintended outcomes.
